{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fortran\n\nThis example demonstrates how to use BlackJAX nested sampling with Fortran implementations of likelihood and prior functions. The Fortran code is compiled to a shared library and accessed via Python's `ctypes` library, with JAX's `pure_callback` providing the bridge.\n\n## Setup Instructions\n\n### 1. Create the Fortran implementation\n\nFirst, create a file `model.f90` with your likelihood and prior functions:"
  },
  {
   "cell_type": "markdown",
   "source": "```fortran\n! model.f90\nmodule model\n    use iso_c_binding\n    implicit none\n    real(c_double), parameter :: LOG_2PI = 1.8378770664093454d0\n    \ncontains\n    \n    ! Scalar likelihood function for a single parameter vector\n    ! Note: One could also define batched functions that process multiple\n    ! parameter vectors in parallel (e.g., using OpenMP directives or\n    ! coarray features) if the likelihood computation is expensive enough\n    ! to benefit from parallelization\n    real(c_double) function loglikelihood_scalar(theta, d) result(logL)\n        real(c_double), intent(in) :: theta(*)\n        integer(c_int), intent(in) :: d\n        real(c_double) :: inv_var, log_det, mu, q, diff\n        integer :: i\n        \n        inv_var = 1.0d0 / 0.01d0\n        log_det = d * log(0.01d0)\n        mu = 1.0d0\n        q = 0.0d0\n        \n        do i = 1, d\n            diff = theta(i) - mu\n            q = q + diff * diff * inv_var\n        end do\n        \n        logL = -0.5d0 * (d * LOG_2PI + log_det + q)\n    end function loglikelihood_scalar\n    \n    ! Scalar prior function for a single parameter vector\n    real(c_double) function logprior_scalar(theta, d) result(logP)\n        real(c_double), intent(in) :: theta(*)\n        integer(c_int), intent(in) :: d\n        real(c_double) :: q\n        integer :: i\n        \n        q = 0.0d0\n        do i = 1, d\n            q = q + theta(i) * theta(i)\n        end do\n        \n        logP = -0.5d0 * (d * LOG_2PI + q)\n    end function logprior_scalar\n    \n    ! Batched likelihood: processes multiple parameter vectors sequentially\n    ! This reduces Python callbacks by a factor of num_delete (typically 50-100),\n    ! which is the dominant cost for fast likelihoods\n    subroutine loglikelihood(theta, result, batch, d) bind(c)\n        real(c_double), intent(in) :: theta(d, batch)\n        real(c_double), intent(out) :: result(batch)\n        integer(c_int), intent(in), value :: batch, d\n        integer :: b\n        \n        ! Simple sequential loop over batch\n        do b = 1, batch\n            result(b) = loglikelihood_scalar(theta(:, b), d)\n        end do\n    end subroutine loglikelihood\n    \n    ! Batched prior: processes multiple parameter vectors sequentially\n    subroutine logprior(theta, result, batch, d) bind(c)\n        real(c_double), intent(in) :: theta(d, batch)\n        real(c_double), intent(out) :: result(batch)\n        integer(c_int), intent(in), value :: batch, d\n        integer :: b\n        \n        ! Simple sequential loop over batch\n        do b = 1, batch\n            result(b) = logprior_scalar(theta(:, b), d)\n        end do\n    end subroutine logprior\n    \nend module model\n```\n\n**Note:** This implementation defines scalar likelihood and prior functions, then provides simple sequential batched versions. While the batching is sequential rather than parallel, this approach is still significantly faster than pure Python (or non-JIT compiled) implementations because it reduces the number of Python callbacks by a factor of `num_delete` (typically 50-100), which is the dominant cost for fast likelihoods.\n\nSave this as `model.f90` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Compile the Fortran code\n\nCompile the Fortran code to a shared library:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```bash\ngfortran -shared -fPIC -O3 -o libmodel.so model.f90\n```",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Create the Python interface\n\nCreate `model.py` to interface with the compiled Fortran library:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```python\n# model.py\nimport ctypes\nimport numpy as np\nfrom numpy.ctypeslib import ndpointer\n\nlib = ctypes.CDLL(\"./libmodel.so\")\n\n# Note: Fortran uses column-major order, so we need F_CONTIGUOUS arrays\nlib.loglikelihood.argtypes = [\n    ndpointer(ctypes.c_double, flags=\"F_CONTIGUOUS\"),\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ctypes.c_int,\n    ctypes.c_int\n]\nlib.loglikelihood.restype = None\n\nlib.logprior.argtypes = [\n    ndpointer(ctypes.c_double, flags=\"F_CONTIGUOUS\"),\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ctypes.c_int,\n    ctypes.c_int\n]\nlib.logprior.restype = None\n\ndef loglikelihood(theta):\n    theta = np.asarray(theta, dtype=np.float64)\n    batch, d = theta.shape\n    # Transpose for Fortran column-major order\n    theta_f = np.asfortranarray(theta.T)\n    result = np.empty(batch, dtype=np.float64)\n    lib.loglikelihood(theta_f, result, batch, d)\n    return result\n\ndef logprior(theta):\n    theta = np.asarray(theta, dtype=np.float64)\n    batch, d = theta.shape\n    # Transpose for Fortran column-major order\n    theta_f = np.asfortranarray(theta.T)\n    result = np.empty(batch, dtype=np.float64)\n    lib.logprior(theta_f, result, batch, d)\n    return result\n```\n\nSave this as `model.py` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Run nested sampling with Fortran functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport blackjax\nfrom blackjax.ns.utils import finalise\nimport tqdm\nimport numpy as np\nimport model  # The Fortran-backed module\n\nrng_key = jax.random.PRNGKey(0)\n\nloglikelihood_fn = model.loglikelihood\nlogprior_fn = model.logprior\n\ndef wrap_fn(fn, vmap_method='legacy_vectorized'):\n    def jax_wrapper(x):\n        out_shape = jax.ShapeDtypeStruct(x.shape[:-1], x.dtype)\n        return jax.pure_callback(fn, out_shape, x, vmap_method=vmap_method)\n    \n    return jax_wrapper\n\nloglikelihood_fn = wrap_fn(loglikelihood_fn)\nlogprior_fn = wrap_fn(logprior_fn)\n\nalgo = blackjax.nss(\n    logprior_fn=logprior_fn,\n    loglikelihood_fn=loglikelihood_fn,\n    num_delete=50,\n    num_inner_steps=20,\n)\n\nrng_key, sampling_key, initialization_key = jax.random.split(rng_key, 3)\nlive = algo.init(jax.random.normal(initialization_key, (1000, 5)))\nstep = jax.jit(algo.step)\n\ndead_points = []\n\nwith tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n    while (not live.logZ_live - live.logZ < -3):\n        rng_key, subkey = jax.random.split(rng_key)\n        live, dead = step(subkey, live)\n        dead_points.append(dead)\n        pbar.update(len(dead.particles))\n\nns_run = finalise(live, dead_points)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}