{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fortran\n\nThis example demonstrates how to use BlackJAX nested sampling with Fortran implementations of likelihood and prior functions. The Fortran code is compiled to a shared library and accessed via Python's `ctypes` library, with JAX's `pure_callback` providing the bridge.\n\n## Prerequisites\n\nInstall the required Python packages:\n```bash\npip install git+https://github.com/handley-lab/blackjax\npip install numpy tqdm\n```\n\nYou'll also need a Fortran compiler (gfortran) installed on your system.\n\n## Setup Instructions\n\n### 1. Create the Fortran implementation\n\nFirst, create a file `model.f90` with your likelihood and prior functions:"
  },
  {
   "cell_type": "markdown",
   "source": "```fortran\n! model.f90\nmodule model_mod\n    use iso_c_binding\n    implicit none\n    real(c_double), parameter :: LOG_2PI = 1.8378770664093454d0\n    \ncontains\n    ! Sequential (scalar) functions\n    function loglikelihood_scalar(theta, d) result(logp)\n        real(c_double), intent(in) :: theta(*)\n        integer(c_int), intent(in), value :: d\n        real(c_double) :: logp\n        \n        real(c_double) :: inv_var, log_det, mu, q, diff\n        integer :: i\n        \n        inv_var = 1.0d0 / 0.01d0\n        log_det = d * log(0.01d0)\n        mu = 1.0d0\n        \n        q = 0.0d0\n        do i = 1, d\n            diff = theta(i) - mu\n            q = q + diff * diff * inv_var\n        end do\n        \n        logp = -0.5d0 * (d * LOG_2PI + log_det + q)\n    end function loglikelihood_scalar\n    \n    function logprior_scalar(theta, d) result(logp)\n        real(c_double), intent(in) :: theta(*)\n        integer(c_int), intent(in), value :: d\n        real(c_double) :: logp\n        \n        real(c_double) :: q\n        integer :: i\n        \n        q = 0.0d0\n        do i = 1, d\n            q = q + theta(i) * theta(i)\n        end do\n        \n        logp = -0.5d0 * (d * LOG_2PI + q)\n    end function logprior_scalar\n    \nend module model_mod\n\n! C-compatible wrapper functions (expecting C row-major order)\nsubroutine loglikelihood(theta, result, batch, d) bind(c)\n    use iso_c_binding\n    use model_mod\n    implicit none\n    \n    integer(c_int), intent(in), value :: batch, d\n    real(c_double), intent(in) :: theta(batch * d)\n    real(c_double), intent(out) :: result(batch)\n    \n    integer :: b, offset\n    \n    do b = 1, batch\n        offset = (b - 1) * d + 1\n        result(b) = loglikelihood_scalar(theta(offset), d)\n    end do\nend subroutine loglikelihood\n\nsubroutine logprior(theta, result, batch, d) bind(c)\n    use iso_c_binding\n    use model_mod\n    implicit none\n    \n    integer(c_int), intent(in), value :: batch, d\n    real(c_double), intent(in) :: theta(batch * d)\n    real(c_double), intent(out) :: result(batch)\n    \n    integer :: b, offset\n    \n    do b = 1, batch\n        offset = (b - 1) * d + 1\n        result(b) = logprior_scalar(theta(offset), d)\n    end do\nend subroutine logprior\n```\n\n**Note:** This implementation defines scalar likelihood and prior functions, then provides simple sequential batched versions. While the batching is sequential rather than parallel, this approach is still significantly faster than pure Python (or non-JIT compiled) implementations because it reduces the number of Python callbacks by a factor of `num_delete` (typically 50-100), which is the dominant cost for fast likelihoods. One could also define batched functions that process multiple parameter vectors in parallel (e.g., using OpenMP directives or coarray features) if the likelihood computation is expensive enough to benefit from parallelization.\n\nSave this as `model.f90` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Compile the Fortran library\n\n```bash\ngfortran -shared -fPIC -O3 -o libmodel.so model.f90\n```",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Create the Python interface\n\nCreate `model.py` to interface with the compiled Fortran library:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```python\n# model.py\nimport ctypes\nimport numpy as np\nfrom numpy.ctypeslib import ndpointer\n\nlib = ctypes.CDLL(\"./libmodel.so\")\n\nlib.loglikelihood.argtypes = [\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ctypes.c_int,\n    ctypes.c_int\n]\nlib.loglikelihood.restype = None\n\nlib.logprior.argtypes = [\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ctypes.c_int,\n    ctypes.c_int\n]\nlib.logprior.restype = None\n\ndef loglikelihood(theta):\n    theta = np.ascontiguousarray(theta, dtype=np.float64)\n    batch, d = theta.shape\n    result = np.empty(batch, dtype=np.float64)\n    lib.loglikelihood(theta, result, batch, d)\n    return result\n\ndef logprior(theta):\n    theta = np.ascontiguousarray(theta, dtype=np.float64)\n    batch, d = theta.shape\n    result = np.empty(batch, dtype=np.float64)\n    lib.logprior(theta, result, batch, d)\n    return result\n```\n\nSave this as `model.py` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Run nested sampling with Fortran functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport blackjax\nfrom blackjax.ns.utils import finalise\nimport tqdm\nimport numpy as np\nimport model  \n\nrng_key = jax.random.PRNGKey(0)\n\ndef wrap_fn(fn, vmap_method='legacy_vectorized'):\n    def jax_wrapper(x):\n        out_shape = jax.ShapeDtypeStruct(x.shape[:-1], x.dtype)\n        return jax.pure_callback(fn, out_shape, x, vmap_method=vmap_method)\n    \n    return jax_wrapper\n\nalgo = blackjax.nss(\n    logprior_fn=wrap_fn(model.logprior),\n    loglikelihood_fn=wrap_fn(model.loglikelihood),\n    num_delete=50,\n    num_inner_steps=20,\n)\n\nrng_key, sampling_key, initialization_key = jax.random.split(rng_key, 3)\nlive = algo.init(jax.random.normal(initialization_key, (1000, 5)))\nstep = jax.jit(algo.step)\n\ndead_points = []\n\nwith tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n    while (not live.logZ_live - live.logZ < -3):\n        rng_key, subkey = jax.random.split(rng_key)\n        live, dead = step(subkey, live)\n        dead_points.append(dead)\n        pbar.update(len(dead.particles))\n\nns_run = finalise(live, dead_points)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}