{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# C++\n\nThis example demonstrates how to use BlackJAX nested sampling with C++ implementations of likelihood and prior functions. The C++ code is compiled using pybind11 to create a Python module, with JAX's `pure_callback` providing the bridge.\n\n## Setup Instructions\n\n### 1. Create the C++ implementation\n\nFirst, create a file `model.cpp` with your likelihood and prior functions using pybind11:"
  },
  {
   "cell_type": "markdown",
   "source": "```cpp\n/* model.cpp */\n#include <pybind11/pybind11.h>\n#include <pybind11/numpy.h>\n#include <cmath>\n\nnamespace py = pybind11;\n\nconstexpr double LOG_2PI = 1.8378770664093454;\n\n// Scalar likelihood function for a single parameter vector\ndouble loglikelihood_scalar(const double* theta, int d) {\n    const double inv_var = 1.0 / 0.01;\n    const double log_det = d * std::log(0.01);\n    const double mu = 1.0;\n    \n    double q = 0.0;\n    for (int i = 0; i < d; i++) {\n        double diff = theta[i] - mu;\n        q += diff * diff * inv_var;\n    }\n    return -0.5 * (d * LOG_2PI + log_det + q);\n}\n\n// Scalar prior function for a single parameter vector\ndouble logprior_scalar(const double* theta, int d) {\n    double q = 0.0;\n    for (int i = 0; i < d; i++) {\n        q += theta[i] * theta[i];\n    }\n    return -0.5 * (d * LOG_2PI + q);\n}\n\n// Batched likelihood: processes multiple parameter vectors sequentially\n// This reduces Python callbacks by a factor of num_delete (typically 50-100),\n// which is the dominant cost for fast likelihoods\npy::array_t<double> loglikelihood(py::array_t<double, py::array::c_style | py::array::forcecast> theta) {\n    py::gil_scoped_release release;  // Release GIL for better performance\n    \n    auto theta_buf = theta.request();\n    int batch = theta_buf.shape[0];\n    int d = theta_buf.shape[1];\n    \n    auto result = py::array_t<double>(batch);\n    auto result_buf = result.request();\n    \n    const double* theta_ptr = static_cast<const double*>(theta_buf.ptr);\n    double* result_ptr = static_cast<double*>(result_buf.ptr);\n    \n    // Simple sequential loop over batch\n    for (int b = 0; b < batch; b++) {\n        result_ptr[b] = loglikelihood_scalar(theta_ptr + b * d, d);\n    }\n    \n    return result;\n}\n\n// Batched prior: processes multiple parameter vectors sequentially\npy::array_t<double> logprior(py::array_t<double, py::array::c_style | py::array::forcecast> theta) {\n    py::gil_scoped_release release;  // Release GIL for better performance\n    \n    auto theta_buf = theta.request();\n    int batch = theta_buf.shape[0];\n    int d = theta_buf.shape[1];\n    \n    auto result = py::array_t<double>(batch);\n    auto result_buf = result.request();\n    \n    const double* theta_ptr = static_cast<const double*>(theta_buf.ptr);\n    double* result_ptr = static_cast<double*>(result_buf.ptr);\n    \n    // Simple sequential loop over batch\n    for (int b = 0; b < batch; b++) {\n        result_ptr[b] = logprior_scalar(theta_ptr + b * d, d);\n    }\n    \n    return result;\n}\n\nPYBIND11_MODULE(model, m) {\n    m.doc() = \"C++ model for BlackJAX nested sampling\";\n    m.def(\"loglikelihood\", &loglikelihood, \"Compute log likelihood\");\n    m.def(\"logprior\", &logprior, \"Compute log prior\");\n}\n```\n\n**Note:** This implementation defines scalar likelihood and prior functions, then provides simple sequential batched versions. While the batching is sequential rather than parallel, this approach is still significantly faster than pure Python (or non-JIT compiled) implementations because it reduces the number of Python callbacks by a factor of `num_delete` (typically 50-100), which is the dominant cost for fast likelihoods.\n\nSave this as `model.cpp` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Create the setup script\n\nCreate a `setup_model_cpp.py` file to compile the C++ module:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```python\n# setup_model_cpp.py\nfrom pybind11.setup_helpers import Pybind11Extension, build_ext\nfrom setuptools import setup\n\next_modules = [\n    Pybind11Extension(\n        \"model\",\n        [\"model.cpp\"],\n        cxx_std=11,\n    ),\n]\n\nsetup(\n    name=\"model\",\n    ext_modules=ext_modules,\n    cmdclass={\"build_ext\": build_ext},\n    zip_safe=False,\n    python_requires=\">=3.7\",\n)\n```\n\nSave this as `setup_model_cpp.py` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Compile the C++ module\n\nInstall pybind11 and compile the module:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```bash\npip install pybind11\npython setup_model_cpp.py build_ext --inplace\n```\n\nThis will create a `model` module that can be imported directly in Python.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Run nested sampling with C++ functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport blackjax\nfrom blackjax.ns.utils import finalise\nimport tqdm\nimport numpy as np\nimport model  # The compiled C++ module\n\nrng_key = jax.random.PRNGKey(0)\n\nloglikelihood_fn = model.loglikelihood\nlogprior_fn = model.logprior\n\ndef wrap_fn(fn, vmap_method='legacy_vectorized'):\n    def jax_wrapper(x):\n        out_shape = jax.ShapeDtypeStruct(x.shape[:-1], x.dtype)\n        return jax.pure_callback(fn, out_shape, x, vmap_method=vmap_method)\n    \n    return jax_wrapper\n\nloglikelihood_fn = wrap_fn(loglikelihood_fn)\nlogprior_fn = wrap_fn(logprior_fn)\n\nalgo = blackjax.nss(\n    logprior_fn=logprior_fn,\n    loglikelihood_fn=loglikelihood_fn,\n    num_delete=50,\n    num_inner_steps=20,\n)\n\nrng_key, sampling_key, initialization_key = jax.random.split(rng_key, 3)\nlive = algo.init(jax.random.normal(initialization_key, (1000, 5)))\nstep = jax.jit(algo.step)\n\ndead_points = []\n\nwith tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n    while (not live.logZ_live - live.logZ < -3):\n        rng_key, subkey = jax.random.split(rng_key)\n        live, dead = step(subkey, live)\n        dead_points.append(dead)\n        pbar.update(len(dead.particles))\n\nns_run = finalise(live, dead_points)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}