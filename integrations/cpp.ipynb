{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# C++\n\nThis example demonstrates how to use BlackJAX nested sampling with C++ implementations of likelihood and prior functions. The C++ code is compiled using pybind11 to create a Python module, with JAX's `pure_callback` providing the bridge.\n\n## Prerequisites\n\nInstall the required Python packages:\n```bash\npip install git+https://github.com/handley-lab/blackjax\npip install pybind11 numpy tqdm\n```\n\nYou'll also need a C++ compiler (g++) installed on your system.\n\n## Setup Instructions\n\n### 1. Create the C++ implementation\n\nFirst, create a file `model.cpp` with your likelihood and prior functions using pybind11:"
  },
  {
   "cell_type": "markdown",
   "source": "```cpp\n/* model.cpp */\n#include <pybind11/pybind11.h>\n#include <pybind11/numpy.h>\n#include <cmath>\n\nnamespace py = pybind11;\n\nconstexpr double LOG_2PI = 1.8378770664093454;\n\n// Sequential (scalar) implementations\ndouble loglikelihood_scalar(py::array_t<double> theta_array) {\n    auto theta = theta_array.unchecked<1>();\n    const int d = theta.shape(0);\n    \n    const double inv_var = 1.0 / 0.01;\n    const double log_det = d * std::log(0.01);\n    const double mu = 1.0;\n    \n    double q = 0.0;\n    for (int i = 0; i < d; ++i) {\n        double diff = theta(i) - mu;\n        q += diff * diff * inv_var;\n    }\n    return -0.5 * (d * LOG_2PI + log_det + q);\n}\n\ndouble logprior_scalar(py::array_t<double> theta_array) {\n    auto theta = theta_array.unchecked<1>();\n    const int d = theta.shape(0);\n    \n    double q = 0.0;\n    for (int i = 0; i < d; ++i) {\n        q += theta(i) * theta(i);\n    }\n    return -0.5 * (d * LOG_2PI + q);\n}\n\n// Sequential (scalar) C++ implementation\nstatic double loglikelihood_scalar_impl(const double* theta, size_t d) {\n    const double inv_var = 1.0 / 0.01;\n    const double log_det = static_cast<double>(d) * std::log(0.01);\n    const double mu = 1.0;\n    \n    double q = 0.0;\n    for (size_t i = 0; i < d; ++i) {\n        double diff = theta[i] - mu;\n        q += diff * diff * inv_var;\n    }\n    return -0.5 * (d * LOG_2PI + log_det + q);\n}\n\nstatic double logprior_scalar_impl(const double* theta, size_t d) {\n    double q = 0.0;\n    for (size_t i = 0; i < d; ++i) {\n        q += theta[i] * theta[i];\n    }\n    return -0.5 * (d * LOG_2PI + q);\n}\n\n// Batched wrappers with GIL release\npy::array_t<double> loglikelihood(py::array_t<double, py::array::c_style | py::array::forcecast> theta) {\n    py::buffer_info info = theta.request();\n    if (info.ndim != 2)\n        throw py::value_error(\"theta must be 2D (batch, dim)\");\n    \n    const size_t batch = static_cast<size_t>(info.shape[0]);\n    const size_t d = static_cast<size_t>(info.shape[1]);\n    const double* data = static_cast<const double*>(info.ptr);\n    \n    py::array_t<double> out(batch);\n    double* out_ptr = out.mutable_data();\n    \n    {\n        py::gil_scoped_release release;\n        for (size_t b = 0; b < batch; ++b) {\n            out_ptr[b] = loglikelihood_scalar_impl(data + b * d, d);\n        }\n    }\n    return out;\n}\n\npy::array_t<double> logprior(py::array_t<double, py::array::c_style | py::array::forcecast> theta) {\n    py::buffer_info info = theta.request();\n    if (info.ndim != 2)\n        throw py::value_error(\"theta must be 2D (batch, dim)\");\n    \n    const size_t batch = static_cast<size_t>(info.shape[0]);\n    const size_t d = static_cast<size_t>(info.shape[1]);\n    const double* data = static_cast<const double*>(info.ptr);\n    \n    py::array_t<double> out(batch);\n    double* out_ptr = out.mutable_data();\n    \n    {\n        py::gil_scoped_release release;\n        for (size_t b = 0; b < batch; ++b) {\n            out_ptr[b] = logprior_scalar_impl(data + b * d, d);\n        }\n    }\n    return out;\n}\n\nPYBIND11_MODULE(model, m) {\n    m.doc() = \"Sequential C++ likelihood and prior functions with batching wrapper\";\n    m.def(\"loglikelihood\", &loglikelihood, \"Log likelihood function (batched)\");\n    m.def(\"logprior\", &logprior, \"Log prior function (batched)\");\n    m.def(\"loglikelihood_scalar\", &loglikelihood_scalar, \"Log likelihood function (scalar)\");\n    m.def(\"logprior_scalar\", &logprior_scalar, \"Log prior function (scalar)\");\n}\n```\n\n**Note:** This implementation defines scalar likelihood and prior functions, then provides simple sequential batched versions. While the batching is sequential rather than parallel, this approach is still significantly faster than pure Python (or non-JIT compiled) implementations because it reduces the number of Python callbacks by a factor of `num_delete` (typically 50-100), which is the dominant cost for fast likelihoods. One could also define batched functions that process multiple parameter vectors in parallel (e.g., using std::execution::par, OpenMP, or SIMD instructions) if the likelihood computation is expensive enough to benefit from parallelization.\n\nSave this as `model.cpp` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Create the setup script\n\nCreate a `setup_model_cpp.py` file to compile the C++ module:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```python\n# setup_model_cpp.py\nfrom pybind11.setup_helpers import Pybind11Extension, build_ext\nfrom setuptools import setup\n\next_modules = [\n    Pybind11Extension(\n        \"model\",\n        [\"model.cpp\"],\n        cxx_std=11,\n    ),\n]\n\nsetup(\n    name=\"model\",\n    ext_modules=ext_modules,\n    cmdclass={\"build_ext\": build_ext},\n    zip_safe=False,\n    python_requires=\">=3.7\",\n)\n```\n\nSave this as `setup_model_cpp.py` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Compile the C++ module\n\nInstall pybind11 and compile the module:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```bash\npip install pybind11\npython setup_model_cpp.py build_ext --inplace\n```\n\nThis will create a `model` module that can be imported directly in Python.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Run nested sampling with C++ functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport blackjax\nfrom blackjax.ns.utils import finalise\nimport tqdm\nimport numpy as np\nimport model  # The compiled C++ module\n\nrng_key = jax.random.PRNGKey(0)\n\nloglikelihood_fn = model.loglikelihood\nlogprior_fn = model.logprior\n\ndef wrap_fn(fn, vmap_method='legacy_vectorized'):\n    def jax_wrapper(x):\n        out_shape = jax.ShapeDtypeStruct(x.shape[:-1], x.dtype)\n        return jax.pure_callback(fn, out_shape, x, vmap_method=vmap_method)\n    \n    return jax_wrapper\n\nloglikelihood_fn = wrap_fn(loglikelihood_fn)\nlogprior_fn = wrap_fn(logprior_fn)\n\nalgo = blackjax.nss(\n    logprior_fn=logprior_fn,\n    loglikelihood_fn=loglikelihood_fn,\n    num_delete=50,\n    num_inner_steps=20,\n)\n\nrng_key, sampling_key, initialization_key = jax.random.split(rng_key, 3)\nlive = algo.init(jax.random.normal(initialization_key, (1000, 5)))\nstep = jax.jit(algo.step)\n\ndead_points = []\n\nwith tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n    while (not live.logZ_live - live.logZ < -3):\n        rng_key, subkey = jax.random.split(rng_key)\n        live, dead = step(subkey, live)\n        dead_points.append(dead)\n        pbar.update(len(dead.particles))\n\nns_run = finalise(live, dead_points)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}