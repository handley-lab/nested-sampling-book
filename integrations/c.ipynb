{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# C\n\nThis example demonstrates how to use BlackJAX nested sampling with C implementations of likelihood and prior functions. The C code is compiled to a shared library and accessed via Python's `ctypes` library, with JAX's `pure_callback` providing the bridge.\n\n## Prerequisites\n\nInstall the required Python packages:\n```bash\npip install git+https://github.com/handley-lab/blackjax\npip install numpy tqdm\n```\n\nYou'll also need a C compiler (gcc) installed on your system.\n\n## Setup Instructions\n\n### 1. Create the C implementation\n\nFirst, create a file `model.c` with your likelihood and prior functions:"
  },
  {
   "cell_type": "markdown",
   "source": "```c\n/* model.c */\n#include <math.h>\n\nstatic const double LOG_2PI = 1.8378770664093454;\n\n// Scalar likelihood function for a single parameter vector\n// Note: One could also define batched functions that process multiple\n// parameter vectors in parallel (e.g., using OpenMP or SIMD instructions)\n// if the likelihood computation is expensive enough to benefit from parallelization\nstatic double loglikelihood_scalar(const double* theta, int d) {\n    const double inv_var = 1.0 / 0.01;\n    const double log_det = d * log(0.01);\n    const double mu = 1.0;\n    \n    double q = 0.0;\n    for (int i = 0; i < d; i++) {\n        double diff = theta[i] - mu;\n        q += diff * diff * inv_var;\n    }\n    return -0.5 * (d * LOG_2PI + log_det + q);\n}\n\n// Scalar prior function for a single parameter vector\nstatic double logprior_scalar(const double* theta, int d) {\n    double q = 0.0;\n    for (int i = 0; i < d; i++) {\n        q += theta[i] * theta[i];\n    }\n    return -0.5 * (d * LOG_2PI + q);\n}\n\n// Batched likelihood: processes multiple parameter vectors sequentially\n// This reduces Python callbacks by a factor of num_delete (typically 50-100),\n// which is the dominant cost for fast likelihoods\nvoid loglikelihood(const double* theta, double* result, int batch, int d) {\n    // Simple sequential loop over batch\n    for (int b = 0; b < batch; b++) {\n        result[b] = loglikelihood_scalar(theta + b * d, d);\n    }\n}\n\n// Batched prior: processes multiple parameter vectors sequentially\nvoid logprior(const double* theta, double* result, int batch, int d) {\n    // Simple sequential loop over batch\n    for (int b = 0; b < batch; b++) {\n        result[b] = logprior_scalar(theta + b * d, d);\n    }\n}\n```\n\n**Note:** This implementation defines scalar likelihood and prior functions, then provides simple sequential batched versions. While the batching is sequential rather than parallel, this approach is still significantly faster than pure Python (or non-JIT compiled) implementations because it reduces the number of Python callbacks by a factor of `num_delete` (typically 50-100), which is the dominant cost for fast likelihoods.\n\nSave this as `model.c` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Compile the C code\n\nCompile the C code to a shared library:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```bash\ngcc -shared -fPIC -O3 -o libmodel.so model.c -lm\n```",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Create the Python interface\n\nCreate `model.py` to interface with the compiled C library:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "```python\n# model.py\nimport ctypes\nimport numpy as np\nfrom numpy.ctypeslib import ndpointer\n\nlib = ctypes.CDLL(\"./libmodel.so\")\n\nlib.loglikelihood.argtypes = [\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ctypes.c_int,\n    ctypes.c_int\n]\nlib.loglikelihood.restype = None\n\nlib.logprior.argtypes = [\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),\n    ctypes.c_int,\n    ctypes.c_int\n]\nlib.logprior.restype = None\n\ndef loglikelihood(theta):\n    theta = np.ascontiguousarray(theta, dtype=np.float64)\n    batch, d = theta.shape\n    result = np.empty(batch, dtype=np.float64)\n    lib.loglikelihood(theta, result, batch, d)\n    return result\n\ndef logprior(theta):\n    theta = np.ascontiguousarray(theta, dtype=np.float64)\n    batch, d = theta.shape\n    result = np.empty(batch, dtype=np.float64)\n    lib.logprior(theta, result, batch, d)\n    return result\n```\n\nSave this as `model.py` in your working directory.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Run nested sampling with C functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport blackjax\nfrom blackjax.ns.utils import finalise\nimport tqdm\nimport numpy as np\nimport model  \n\nrng_key = jax.random.PRNGKey(0)\n\nloglikelihood_fn = model.loglikelihood\nlogprior_fn = model.logprior\n\ndef wrap_fn(fn, vmap_method='legacy_vectorized'):\n    def jax_wrapper(x):\n        out_shape = jax.ShapeDtypeStruct(x.shape[:-1], x.dtype)\n        return jax.pure_callback(fn, out_shape, x, vmap_method=vmap_method)\n    \n    return jax_wrapper\n\nloglikelihood_fn = wrap_fn(loglikelihood_fn)\nlogprior_fn = wrap_fn(logprior_fn)\n\nalgo = blackjax.nss(\n    logprior_fn=logprior_fn,\n    loglikelihood_fn=loglikelihood_fn,\n    num_delete=50,\n    num_inner_steps=20,\n)\n\nrng_key, sampling_key, initialization_key = jax.random.split(rng_key, 3)\nlive = algo.init(jax.random.normal(initialization_key, (1000, 5)))\nstep = jax.jit(algo.step)\n\ndead_points = []\n\nwith tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n    while (not live.logZ_live - live.logZ < -3):\n        rng_key, subkey = jax.random.split(rng_key)\n        live, dead = step(subkey, live)\n        dead_points.append(dead)\n        pbar.update(len(dead.particles))\n\nns_run = finalise(live, dead_points)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}