{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Neural Likelihood Estimation with Nested Sampling\n\nThis example demonstrates how to use BlackJAX nested sampling as a posterior sampler for simulation-based inference (SBI) with neural likelihood estimation (NLE).\n\n## Prerequisites\n\nInstall the required packages:\n```bash\npip install git+https://github.com/handley-lab/blackjax\npip install sbi torch anesthetic numpy tqdm\n```\n\n## NSPosterior Implementation\n\nFirst, we define a custom posterior class that uses BlackJAX nested sampling to sample from the posterior distribution given a trained likelihood estimator and prior."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import Optional\n\nimport torch\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport blackjax\nfrom blackjax.ns.utils import finalise\nimport anesthetic\nimport tqdm\nfrom sbi.inference.posteriors.base_posterior import NeuralPosterior\nfrom sbi.inference.potentials.likelihood_based_potential import LikelihoodBasedPotential\n\n\nclass NSPosterior(NeuralPosterior):\n    \"\"\"Nested Sampling posterior for sbi.\n    \n    Uses BlackJAX nested sampling to sample from posterior distributions\n    when given a likelihood estimator and prior.\n    \n    Args:\n        likelihood: Trained likelihood estimator from NLE\n        prior: Prior distribution\n        num_live: Number of live points for nested sampling\n        num_inner_steps: Number of slice sampling steps\n        x_o: Observed data (can be set later with set_default_x)\n        num_delete: Number of points to delete per iteration (default: 1)\n    \"\"\"\n    \n    def __init__(\n        self,\n        likelihood,\n        prior,\n        num_live,\n        num_inner_steps,\n        x_o=None,\n        num_delete=1,\n    ):\n        self.num_live = num_live\n        self.num_delete = num_delete\n        self.num_inner_steps = num_inner_steps\n        \n        potential_fn = LikelihoodBasedPotential(likelihood, prior, x_o)\n        super().__init__(potential_fn)\n    \n    def _loglikelihood_fn(self, theta):\n        x_o_batch = self.default_x.unsqueeze(0).expand(1, theta.shape[0], -1)\n        return self.potential_fn.likelihood_estimator.log_prob(x_o_batch, condition=theta).squeeze(0)\n    \n    def _logprior_fn(self, theta):\n        return self.potential_fn.prior.log_prob(theta)\n    \n    def nested_samples(self) -> anesthetic.NestedSamples:\n        \"\"\"Run BlackJAX nested sampling and return anesthetic NestedSamples object.\n        \n        Returns:\n            NestedSamples object containing particles, log-likelihoods, and birth \n            log-likelihoods that can be used for evidence calculation and diagnostics.\n        \"\"\"\n        def wrap_fn(fn, vmap_method='legacy_vectorized'):\n            def numpy_wrapper(theta):\n                x = torch.from_numpy(np.asarray(theta).copy()).float()\n                result = fn(x)\n                return result.detach().numpy()\n            \n            def jax_wrapper(x):\n                out_shape = jax.ShapeDtypeStruct(x.shape[:-1], x.dtype)\n                return jax.pure_callback(numpy_wrapper, out_shape, x, vmap_method=vmap_method)\n            \n            return jax_wrapper\n        \n        algo = blackjax.nss(\n            logprior_fn=wrap_fn(self._logprior_fn),\n            loglikelihood_fn=wrap_fn(self._loglikelihood_fn),\n            num_delete=self.num_delete,\n            num_inner_steps=self.num_inner_steps,\n        )\n        prior_samples = self.potential_fn.prior.sample((self.num_live,))\n        initial_live = jnp.array(prior_samples.numpy())\n        \n        rng_key = jax.random.PRNGKey(42)\n        live = algo.init(initial_live)\n        step = jax.jit(algo.step)\n        \n        dead_points = []\n        \n        with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n            while (not live.logZ_live - live.logZ < -3):\n                rng_key, subkey = jax.random.split(rng_key)\n                live, dead = step(subkey, live)\n                dead_points.append(dead)\n                pbar.update(len(dead.particles))\n\n        ns_run = finalise(live, dead_points)\n        \n        return anesthetic.NestedSamples(\n            data=ns_run.particles,\n            logL=ns_run.loglikelihood,\n            logL_birth=ns_run.loglikelihood_birth,\n        )\n    \n    def sample(\n        self,\n        sample_shape: torch.Size = torch.Size(),\n        x: Optional[torch.Tensor] = None,\n        **kwargs\n    ) -> torch.Tensor:\n        r\"\"\"Return samples from posterior distribution $p(\\theta|x)$ with nested sampling.\n        \n        Check the `__init__()` method for a description of all arguments as well as\n        their default values.\n        \n        Args:\n            sample_shape: Desired shape of samples that are drawn from posterior. If\n                sample_shape is multidimensional we simply draw `sample_shape.numel()`\n                samples and then reshape into the desired shape.\n                \n        Returns:\n            Samples from posterior with shape `(*sample_shape, theta_dim)`.\n        \"\"\"\n        self.potential_fn.set_x(self._x_else_default_x(x))\n        ns = self.nested_samples()\n        samples = ns.sample(torch.Size(sample_shape).numel())\n        samples_array = samples.drop(columns=['logL', 'logL_birth', 'nlive']).values\n        return torch.from_numpy(samples_array).reshape((*sample_shape, -1))\n    \n    \n    def sample_batched(self, sample_shape, x, **kwargs):\n        raise NotImplementedError(\"Batched sampling not available for nested sampling.\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Example: Neural Likelihood Estimation\n\nNow let's use the NSPosterior class with neural likelihood estimation (NLE) from the sbi package.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sbi.inference import NLE\nfrom sbi.utils import BoxUniform\n\nnum_dims = 2\nnum_sims = 1000\nnum_rounds = 2\nprior = BoxUniform(low=torch.zeros(num_dims), high=torch.ones(num_dims))\nsimulator = lambda theta: theta + torch.randn_like(theta) * 0.1\nx_o = torch.tensor([0.5, 0.5])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sequential Neural Likelihood Estimation\n\nTrain the likelihood estimator over multiple rounds, using nested sampling to generate proposal samples.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "inference = NLE(prior)\nproposal = prior\nfor _ in range(num_rounds):\n    theta = proposal.sample((num_sims,))\n    x = simulator(theta)\n    likelihood = inference.append_simulations(theta, x).train()\n    posterior = NSPosterior(likelihood, prior, num_live=500, num_inner_steps=20, num_delete=250)\n    proposal = posterior.set_default_x(x_o)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Analyze Results\n\nGet the nested samples and analyze the posterior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "nested_samples = posterior.nested_samples()\nnested_samples.to_csv(\"nested_samples.csv\")\nprint(nested_samples)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}