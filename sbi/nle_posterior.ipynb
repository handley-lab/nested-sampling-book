{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Neural Likelihood Estimation with Nested Sampling\n\nThis example demonstrates how to use BlackJAX nested sampling as a posterior sampler for simulation-based inference (SBI) with neural likelihood estimation (NLE).\n\n## Prerequisites\n\nInstall the required packages:\n```bash\npip install git+https://github.com/handley-lab/blackjax\npip install sbi torch anesthetic numpy tqdm\n```\n\n## NSPosterior Implementation\n\nFirst, we define a custom posterior class that uses BlackJAX nested sampling to sample from the posterior distribution given a trained likelihood estimator and prior."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport blackjax\nfrom blackjax.ns.utils import finalise\nimport anesthetic\nimport tqdm\n\n\nclass NSPosterior:\n    \"\"\"Nested Sampling posterior for sbi.\n    \n    Uses BlackJAX nested sampling to sample from posterior distributions\n    when given a likelihood estimator and prior.\n    \n    Args:\n        likelihood: Trained likelihood estimator from NLE\n        prior: Prior distribution\n        num_live: Number of live points for nested sampling\n        num_inner_steps: Number of slice sampling steps\n        x_o: Observed data (can be set later with set_default_x)\n        num_delete: Number of points to delete per iteration (default: 1)\n    \"\"\"\n    \n    def __init__(\n        self,\n        likelihood,\n        prior,\n        num_live,\n        num_inner_steps,\n        x_o=None,\n        num_delete=1,\n    ):\n        self.likelihood = likelihood\n        self.prior = prior\n        self._x = x_o\n        self.num_live = num_live\n        self.num_delete = num_delete\n        self.num_inner_steps = num_inner_steps\n    \n    @property\n    def default_x(self):\n        \"\"\"Return default x used by .sample(), .log_prob() as conditioning context.\"\"\"\n        return self._x\n    \n    @default_x.setter\n    def default_x(self, x):\n        \"\"\"See `set_default_x`.\"\"\"\n        self.set_default_x(x)\n    \n    def set_default_x(self, x):\n        \"\"\"Set new default x for .sample(), .log_prob() to use as conditioning context.\n        \n        This convenience is particularly useful when the posterior is focused, i.e.\n        has been trained over multiple rounds to be accurate in the vicinity of a\n        particular x=x_o.\n        \n        NOTE: this method is chainable, i.e. will return the NSPosterior object so\n        that calls like posterior.set_default_x(my_x).sample(mytheta) are possible.\n        \n        Args:\n            x: The default observation to set for the posterior p(Î¸|x).\n        Returns:\n            NSPosterior that will use a default x when not explicitly passed.\n        \"\"\"\n        self._x = x\n        return self\n    \n    def _x_else_default_x(self, x):\n        if x is not None:\n            return x\n        elif self.default_x is None:\n            raise ValueError(\n                \"Context `x` needed when a default has not been set.\"\n                \"If you'd like to have a default, use the `.set_default_x()` method.\"\n            )\n        else:\n            return self.default_x\n    \n    def _loglikelihood_fn(self, theta):\n        x_o_batch = self.default_x.unsqueeze(0).expand(1, theta.shape[0], -1)\n        return self.likelihood.log_prob(x_o_batch, condition=theta).squeeze(0)\n    \n    def _logprior_fn(self, theta):\n        return self.prior.log_prob(theta)\n    \n    def nested_samples(self):\n        \"\"\"Run BlackJAX nested sampling.\"\"\"\n        def wrap_fn(fn, vmap_method='legacy_vectorized'):\n            def numpy_wrapper(theta):\n                x = torch.from_numpy(np.asarray(theta).copy()).float()\n                result = fn(x)\n                return result.detach().numpy()\n            \n            def jax_wrapper(x):\n                out_shape = jax.ShapeDtypeStruct(x.shape[:-1], x.dtype)\n                return jax.pure_callback(numpy_wrapper, out_shape, x, vmap_method=vmap_method)\n            \n            return jax_wrapper\n        \n        algo = blackjax.nss(\n            logprior_fn=wrap_fn(self._logprior_fn),\n            loglikelihood_fn=wrap_fn(self._loglikelihood_fn),\n            num_delete=self.num_delete,\n            num_inner_steps=self.num_inner_steps,\n        )\n        prior_samples = self.prior.sample((self.num_live,))\n        initial_live = jnp.array(prior_samples.numpy())\n        \n        rng_key = jax.random.PRNGKey(42)\n        live = algo.init(initial_live)\n        step = jax.jit(algo.step)\n        \n        dead_points = []\n        \n        with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n            while (not live.logZ_live - live.logZ < -3):\n                rng_key, subkey = jax.random.split(rng_key)\n                live, dead = step(subkey, live)\n                dead_points.append(dead)\n                pbar.update(len(dead.particles))\n\n        ns_run = finalise(live, dead_points)\n        \n        return anesthetic.NestedSamples(\n            data=ns_run.particles,\n            logL=ns_run.loglikelihood,\n            logL_birth=ns_run.loglikelihood_birth,\n        )\n    \n    def sample(self, sample_shape=torch.Size(), x=None):\n        \"\"\"Return unweighted posterior samples.\n        \n        Args:\n            sample_shape: Desired shape of samples\n            x: Optional observation (uses default if not provided)\n            \n        Returns:\n            PyTorch tensor of samples with shape (*sample_shape, param_dim)\n        \"\"\"\n        x_o = self._x_else_default_x(x)\n        if x is not None:\n            # Temporarily set x for sampling\n            old_x = self._x\n            self._x = x_o\n            ns = self.nested_samples()\n            self._x = old_x\n        else:\n            ns = self.nested_samples()\n        samples = ns.sample(torch.Size(sample_shape).numel())\n        samples_array = samples.drop(columns=['logL', 'logL_birth', 'nlive']).values\n        return torch.from_numpy(samples_array).reshape((*sample_shape, -1))\n    \n    def log_prob(self, theta, x=None):\n        \"\"\"Evaluate unnormalized log posterior.\n        \n        Args:\n            theta: Parameters to evaluate\n            x: Optional observation (uses default if not provided)\n            \n        Returns:\n            Log posterior values (unnormalized)\n        \"\"\"\n        x_o = self._x_else_default_x(x)\n        if x is not None:\n            # Temporarily set x for evaluation\n            old_x = self._x\n            self._x = x_o\n            result = self._loglikelihood_fn(torch.as_tensor(theta)) + self._logprior_fn(torch.as_tensor(theta))\n            self._x = old_x\n            return result\n        else:\n            theta = torch.as_tensor(theta)\n            return self._loglikelihood_fn(theta) + self._logprior_fn(theta)"
  },
  {
   "cell_type": "markdown",
   "source": "## Example: Neural Likelihood Estimation\n\nNow let's use the NSPosterior class with neural likelihood estimation (NLE) from the sbi package.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sbi.inference import NLE\nfrom sbi.utils import BoxUniform\n\n# Define the prior\nnum_dims = 2\nnum_sims = 1000\nnum_rounds = 2\nprior = BoxUniform(low=torch.zeros(num_dims), high=torch.ones(num_dims))\n\n# Simple simulator: adds Gaussian noise to parameters\nsimulator = lambda theta: theta + torch.randn_like(theta) * 0.1\n\n# Observed data\nx_o = torch.tensor([0.5, 0.5])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sequential Neural Likelihood Estimation\n\nTrain the likelihood estimator over multiple rounds, using nested sampling to generate proposal samples.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "inference = NLE(prior)\nproposal = prior\n\nfor round_idx in range(num_rounds):\n    print(f\"\\nRound {round_idx + 1}/{num_rounds}\")\n    \n    # Sample from proposal\n    theta = proposal.sample((num_sims,))\n    \n    # Simulate data\n    x = simulator(theta)\n    \n    # Train likelihood estimator\n    likelihood = inference.append_simulations(theta, x).train()\n    \n    # Create nested sampling posterior\n    posterior = NSPosterior(\n        likelihood, \n        prior, \n        num_live=500, \n        num_inner_steps=20, \n        num_delete=250\n    )\n    \n    # Set observation and use as proposal for next round\n    proposal = posterior.set_default_x(x_o)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Analyze Results\n\nGet the nested samples and analyze the posterior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Get nested samples\nnested_samples = posterior.nested_samples()\n\n# Save to CSV for analysis\nnested_samples.to_csv(\"nested_samples.csv\")\n\n# Display summary statistics\nprint(\"\\nNested Sampling Results:\")\nprint(f\"Log evidence: {nested_samples.logZ():.2f}\")\nprint(f\"Number of samples: {len(nested_samples)}\")\nprint(f\"\\nPosterior mean:\")\nprint(nested_samples.mean())\nprint(f\"\\nPosterior std:\")\nprint(nested_samples.std())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}